{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Interview Questions with answers and code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :\n",
    "\n",
    "Deep learning is a type of artificial intelligence that involves using complex algorithms called neural networks to process data, learn patterns, and make decisions. These neural networks are designed to mimic the way the human brain operates, allowing machines to recognize and respond to complex patterns in a similar way to humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: How does Deep Learning differ from traditional Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a subset of machine learning that uses multi-layered neural networks to analyze various factors of data. Unlike traditional machine learning, which often requires manual feature extraction and selection, deep learning automatically learns features from raw data, making it highly effective for complex tasks like image and speech recognition. This allows deep learning to handle larger datasets and more complex problems more effectively than traditional machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a series of algorithms that attempts to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In essence, it is a system of interconnected \"neurons\" that can process inputs, weight them, and produce outputs. These networks are typically organized in layers, with each layer performing different transformations on the input data to help the system learn complex patterns for tasks such as classification, regression, and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Explain the concept of a neuron in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, a neuron is a fundamental unit that simulates the behavior of biological neurons in the human brain. It receives input, processes it, and generates output based on that input. Each neuron in a neural network receives multiple inputs, applies weights to them, sums them up, and passes the sum through an activation function to produce an output. This output can then serve as an input to the next layer of neurons in the network. The process allows neural networks to learn complex patterns from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Explain architecture of Neural Networks in simple way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are structured similarly to the human brain. They consist of layers of neurons, which are simple computational units. The architecture of NN is as follows:\n",
    "\n",
    "1. **Input Layer**: This is where the data enters the network. Each neuron in this layer represents a feature of the input data.\n",
    "\n",
    "2. **Hidden Layers**: These layers are between the input and output. They process the inputs from the previous layer using weights (which are learned during training) and biases, often passed through an activation function to introduce non-linearity.\n",
    "\n",
    "3. **Output Layer**: The final layer produces the network’s predictions, formatted to suit the specific type of problem (like classification or regression).\n",
    "\n",
    "Data flows from the input layer through the hidden layers to the output layer, and during training, the network adjusts the weights and biases to minimize error in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: What is an activation function in a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function in a neural network is a mathematical operation applied to each neuron's output in the network. It determines whether the neuron should be activated or not, helping to add non-linearity to the decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Name few popular activationfunctions and describe them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Activation Functions:\n",
    "\n",
    "**Sigmoid**: Outputs values between 0 and 1, making it useful for models where we need to predict probabilities as outputs.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: Provides output x if x is positive and 0 otherwise. It is the most commonly used activation function in neural networks due to its computational efficiency and the ability to handle vanishing gradient problems better than sigmoid.\n",
    "\n",
    "**Tanh (Hyperbolic Tangent)**: Outputs values between -1 and 1, which centers the data thus aiding in data preprocessing. It is similar to sigmoid but provides a larger output range.\n",
    "\n",
    "**Softmax**: Used in the output layer of a neural network to perform multi-class classification; it returns probability scores for a set of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: What happens if you do not use any activation functions in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "If you do not use any activation functions in a neural network, essentially, each layer in the network would only perform a linear transformation on the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: What is Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. Gradient Descent is a fundamental optimization algorithm. how Gradient Descent works:\n",
    "\n",
    "1- Initialization:\n",
    "Start with initial values for the parameters.\n",
    "\n",
    "2- Compute the Gradient: \n",
    "Calculate the gradient of the loss function with respect to each parameter. The gradient is a vector that points in the direction of the steepest increase of the loss function.\n",
    "\n",
    "3- Update the Parameters: \n",
    "Adjust the parameters in the opposite direction of the gradient to reduce the loss.\n",
    "\n",
    "4- Iterate: \n",
    "Repeat the process of computing the gradient and updating the parameters until the algorithm converges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: What is the function of anoptimizer in Deep Learning?\n",
    "### Answer:\n",
    "In deep learning, an optimizer is a critical component used to update the parameters (weights and biases) of a neural network to minimize the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: How is backpropagation different from gradient descent?\n",
    "### Answer\n",
    "Backpropagation and gradient descent are two integral, but distinct, concepts in the training of neural networks. \n",
    "\n",
    "1- Backpropagation is a method used for calculating the gradient of the loss function of a neural network with respect to its weights and biases. It is essentially a specific application of the chain rule from calculus to efficiently compute these gradients across all layers in a network. \n",
    "\n",
    "2- Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the network’s weights and biases in the direction that oppositely correlates with the gradient computed during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Describe what Vanishing Gradient Problem is and it's impact on NN\n",
    "### Answer\n",
    "1- The vanishing gradient problem refers to the issue of diminishing gradients during the training of deep neural networks. It occurs when the gradients propagated backward through the layers become very small, making it difficult for the network to update the weights effectively.\n",
    "\n",
    "2- it's impact on NN\n",
    "\n",
    "Slow Convergence: When gradients vanish, the updates to weights in the earlier layers become very small.\n",
    "\n",
    "Poor Performance: Since the early layers fail to learn effectively, they don’t capture useful features from the input data, potentially resulting in poor overall performance of the network.\n",
    "\n",
    "Initialization and Activation Dependency: The problem tends to be more severe depending on the choice of weight initialization and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: Mitigating the Vanishing Gradient Problem\n",
    "### Answer\n",
    "1- ReLU Activation Function: Using the Rectified Linear Unit (ReLU) and variants like Leaky ReLU or Parametric ReLU helps prevent vanishing gradients because the derivative of ReLU is 1 for all positive inputs, ensuring that gradients do not diminish as quickly.\n",
    "\n",
    "2- Use of Residual Networks: Architectures like ResNets introduce skip connections that allow gradients to flow through the network more directly, mitigating the vanishing problem by providing alternate pathways for gradient propagation.\n",
    "\n",
    "3- Batch Normalization: This technique normalizes the input layer by adjusting and scaling activations, which helps maintain a healthy gradient flow across deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: There is a neuron in the hidden layer that always results in an error.\n",
    "### Answer\n",
    " it suggests that there might be an issue with how the neuron is functioning or being integrated into the network. This can arise from various sources such as problems in the neuron's activation, issues with weight initialization, or errors in the data feeding into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: What do you understand by a computational graph?\n",
    "### Answer\n",
    "A computational graph is a directed graph where the nodes correspond to operations or variables. Variables can feed their value into operations, and operations can feed their output into other operations. This way, every node in the graph defines a function of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: What is Cross Entropy loss function\n",
    "### Answer\n",
    "Cross Entropy Loss function, also known as log loss, is a widely used performance metric for classification models, particularly in settings where the outputs are probabilities. It measures the performance of a classification model whose output is a probability value between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17: Why is Cross-entropy preferred as the cost function for multi-class classification problems?\n",
    "# Answer\n",
    "Because the decision boundary in a classification task is large (in comparison with regression).\n",
    "Cross-entropy is preferred as the cost function for multi-class classification problems due to its effectiveness in handling probabilities and its impact on the training process of a classifier. This preference stems from several key properties and advantages that make cross-entropy particularly suitable for tasks where predictions are inherently probabilistic and classes are mutually exclusive.\n",
    "\n",
    "1- Cross-entropy loss provides useful gradients during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18: How can optimization methods like gradient descent be improved? \n",
    "### Answer\n",
    "\n",
    "Momentum can accelerate the convergence and reduce the oscillations of the gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19: Compare batch gradient descent, minibatch gradient descent, and stochastic gradient descent.\n",
    "### Answer:\n",
    "In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step.\n",
    "\n",
    "In Batch Gradient Descent we were considering all the examples for every step of Gradient Descent. \n",
    "\n",
    "But, since in SGD we use only one example at a time, we cannot implement the vectorized implementation on it. This can slow down the computations. To tackle this problem, a mixture of Batch Gradient Descent and SGD is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 20: What kind of loss function should we use for multi class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use multi-class ceoss entropy which is known SofMax function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 21: How to decide batch size in deep learning (considering both too small and too large sizes)\n",
    "### Answer:\n",
    "Smaller batches offer faster convergence but may introduce noisy gradients, while larger batches provide smoother gradients but may slow convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 22: Batch Size vs Model Performance: How does the batch size impact the performance of a deep learning model?\n",
    "### Answer:\n",
    "Choosing the optimal batch size for training a deep learning model is a critical decision that can significantly influence the efficiency, effectiveness, and outcome of the training process. \n",
    "A larger batch size can lead to faster convergence as the model updates its parameters less frequently but with more data at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 23: What is Hessian, and how can it be used for faster training? What are its disadvantages?\n",
    "### Answer:\n",
    "The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. In the context of machine learning, particularly in optimization problems, the Hessian matrix describes the local curvature of the loss function being minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 24: What is RMSProp and how does it work?\n",
    "### Answer:\n",
    "RMSProp is a powerful optimization algorithm. Its adaptive learning rates help overcome some of the challenges faced by SGD, leading to faster convergence and improved stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 25: Discuss the concept of an adaptive learning rate. Describe adaptive learning methods\n",
    "### Answer:\n",
    "Adaptive learning rate methods are an optimization of gradient descent methods with the goal of minimizing the objective function of a network by using the gradient of the function and the parameters of the network.\n",
    "\n",
    "    1. AdaGrad (Adaptive Gradient Algorithm)\n",
    "    How it works: AdaGrad adapts the learning rate to the parameters by performing larger updates for infrequent parameters and smaller updates for frequent ones. \n",
    "    2. RMSprop (Root Mean Square Propagation)\n",
    "    How it works: RMSprop modifies AdaGrad's aggressive, monotonically decreasing learning rate by using a moving average of squared gradients. \n",
    "    3. Adam (Adaptive Moment Estimation)\n",
    "    How it works: Adam combines ideas from both momentum and RMSprop. It maintains an exponentially decaying average of past gradients (momentum) and squared gradients (uncentered variance). Adam adjusts the learning rate for each parameter based on these first and second moment estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 26: What is AdamW and why it's preferred over Adam?\n",
    "### Answer:\n",
    "AdamW is a modification of the Adam optimizer that incorporates a better handling of weight decay.\n",
    "Weight decay is a regularization technique used in training machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 27: what is l1 and l2 regularization and difference between them?\n",
    "### Answer:\n",
    "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights. The L1 regularization solution is sparse. The L2 regularization solution is non-sparse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 28: What is Batch Normalization and why it's used in NN?\n",
    "### Answer:\n",
    "Batch Normalization (BatchNorm) is a method applied to individual layers in a neural network. It normalizes the activations of the previous layer at each batch, i.e., it applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. \n",
    "\n",
    "Batch Normalization offers several benefits in training deep neural networks:\n",
    "\n",
    "1- Improves Gradient Flow: By normalizing the inputs across mini-batches, Batch Normalization helps in maintaining a stable gradient flow throughout the training process, which can lead to faster convergence.\n",
    "\n",
    "2- Allows Higher Learning Rates: Batch Normalization stabilizes the learning process, allowing for higher learning rates without the risk of divergence.\n",
    "\n",
    "3- Reduces the Dependence on Initialization: Since the inputs to each layer are normalized, the network becomes less sensitive to the initial parameters' values.\n",
    "\n",
    "4- Acts as a Regularizer: In some cases, Batch Normalization has been shown to act as a regularizer, reducing (or even eliminating) the need for Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 28: What is Layer Normalization, and why it's used in NN?\n",
    "### Answer:\n",
    "Layer Normalization is a technique designed to normalize the inputs across the features for each training example. Unlike Batch Normalization, which normalizes across the batch dimension for each feature independently, Layer Normalization performs the normalization across the features for each individual data point in a batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 28: Wwhat is Knowledge Distillation and how loss function is defined between teacher and student model?\n",
    "#### Answer:\n",
    "Knowledge Distillation is a technique used to transfer the knowledge from a large, complex model (referred to as the \"teacher\") to a smaller, simpler model (referred to as the \"student\"). \n",
    "The basic idea behind knowledge distillation is to use the output probabilities (class predictions) of the teacher model as soft targets for training the student model. Here’s a step-by-step explanation:\n",
    "\n",
    "Teacher Model Training: First, a large and typically very deep or complex model is trained on a given dataset. This model achieves high performance but is often too computationally expensive or slow for deployment in resource-constrained environments.\n",
    "\n",
    "Generate Soft Targets: The teacher model's output probabilities, often obtained using a softmax function on the logits, are softened using a temperature parameter 𝑇. \n",
    "\n",
    "Student Model Training: The student model, which is smaller and less complex, is then trained not only on the hard targets (true labels) but also to match these soft targets from the teacher model. The intuition here is that the soft targets carry much more information per example than hard targets (e.g., about the relationships between different classes).\n",
    "\n",
    "The loss function in knowledge distillation typically combines two components:\n",
    "\n",
    "Distillation Loss: This part of the loss measures how closely the student model's predicted probabilities (softened by the same temperature 𝑇) match the softened probabilities from the teacher model.  It uses the Kullback-Leibler divergence.\n",
    "\n",
    "Standard Cross-Entropy Loss: This is the typical cross-entropy loss between the hard labels and the student's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example is in test_softmax.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
