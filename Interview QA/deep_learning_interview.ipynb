{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Interview Questions with answers and code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :\n",
    "\n",
    "Deep learning is a type of artificial intelligence that involves using complex algorithms called neural networks to process data, learn patterns, and make decisions. These neural networks are designed to mimic the way the human brain operates, allowing machines to recognize and respond to complex patterns in a similar way to humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: How does Deep Learning differ from traditional Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a subset of machine learning that uses multi-layered neural networks to analyze various factors of data. Unlike traditional machine learning, which often requires manual feature extraction and selection, deep learning automatically learns features from raw data, making it highly effective for complex tasks like image and speech recognition. This allows deep learning to handle larger datasets and more complex problems more effectively than traditional machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a series of algorithms that attempts to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In essence, it is a system of interconnected \"neurons\" that can process inputs, weight them, and produce outputs. These networks are typically organized in layers, with each layer performing different transformations on the input data to help the system learn complex patterns for tasks such as classification, regression, and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Explain the concept of a neuron in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, a neuron is a fundamental unit that simulates the behavior of biological neurons in the human brain. It receives input, processes it, and generates output based on that input. Each neuron in a neural network receives multiple inputs, applies weights to them, sums them up, and passes the sum through an activation function to produce an output. This output can then serve as an input to the next layer of neurons in the network. The process allows neural networks to learn complex patterns from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Explain architecture of Neural Networks in simple way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are structured similarly to the human brain. They consist of layers of neurons, which are simple computational units. The architecture of NN is as follows:\n",
    "\n",
    "1. **Input Layer**: This is where the data enters the network. Each neuron in this layer represents a feature of the input data.\n",
    "\n",
    "2. **Hidden Layers**: These layers are between the input and output. They process the inputs from the previous layer using weights (which are learned during training) and biases, often passed through an activation function to introduce non-linearity.\n",
    "\n",
    "3. **Output Layer**: The final layer produces the network’s predictions, formatted to suit the specific type of problem (like classification or regression).\n",
    "\n",
    "Data flows from the input layer through the hidden layers to the output layer, and during training, the network adjusts the weights and biases to minimize error in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: What is an activation function in a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function in a neural network is a mathematical operation applied to each neuron's output in the network. It determines whether the neuron should be activated or not, helping to add non-linearity to the decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Name few popular activationfunctions and describe them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Activation Functions:\n",
    "\n",
    "**Sigmoid**: Outputs values between 0 and 1, making it useful for models where we need to predict probabilities as outputs.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: Provides output x if x is positive and 0 otherwise. It is the most commonly used activation function in neural networks due to its computational efficiency and the ability to handle vanishing gradient problems better than sigmoid.\n",
    "\n",
    "**Tanh (Hyperbolic Tangent)**: Outputs values between -1 and 1, which centers the data thus aiding in data preprocessing. It is similar to sigmoid but provides a larger output range.\n",
    "\n",
    "**Softmax**: Used in the output layer of a neural network to perform multi-class classification; it returns probability scores for a set of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: What happens if you do not use any activation functions in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "If you do not use any activation functions in a neural network, essentially, each layer in the network would only perform a linear transformation on the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: What is Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm. how Gradient Descent works:\n",
    "\n",
    "1- Initialization:\n",
    "Start with initial values for the parameters.\n",
    "\n",
    "2- Compute the Gradient: \n",
    "Calculate the gradient of the loss function with respect to each parameter. The gradient is a vector that points in the direction of the steepest increase of the loss function.\n",
    "\n",
    "3- Update the Parameters: \n",
    "Adjust the parameters in the opposite direction of the gradient to reduce the loss.\n",
    "\n",
    "4- Iterate: \n",
    "Repeat the process of computing the gradient and updating the parameters until the algorithm converges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: What is the function of anoptimizer in Deep Learning?\n",
    "### Answer:\n",
    "In deep learning, an optimizer is a critical component used to update the parameters (weights and biases) of a neural network to minimize the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: How is backpropagation different from gradient descent?\n",
    "### Answer\n",
    "Backpropagation and gradient descent are two integral, but distinct, concepts in the training of neural networks. \n",
    "\n",
    "1- Backpropagation is a method used for calculating the gradient of the loss function of a neural network with respect to its weights and biases. It is essentially a specific application of the chain rule from calculus to efficiently compute these gradients across all layers in a network. \n",
    "\n",
    "2- Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the network’s weights and biases in the direction that oppositely correlates with the gradient computed during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Describe what Vanishing Gradient Problem is and it's impact on NN\n",
    "### Answer\n",
    "1- The vanishing gradient problem refers to the issue of diminishing gradients during the training of deep neural networks. It occurs when the gradients propagated backward through the layers become very small, making it difficult for the network to update the weights effectively.\n",
    "\n",
    "2- it's impact on NN\n",
    "\n",
    "Slow Convergence: When gradients vanish, the updates to weights in the earlier layers become very small.\n",
    "\n",
    "Poor Performance: Since the early layers fail to learn effectively, they don’t capture useful features from the input data, potentially resulting in poor overall performance of the network.\n",
    "\n",
    "Initialization and Activation Dependency: The problem tends to be more severe depending on the choice of weight initialization and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: Mitigating the Vanishing Gradient Problem\n",
    "### Answer\n",
    "1- ReLU Activation Function: Using the Rectified Linear Unit (ReLU) and variants like Leaky ReLU or Parametric ReLU helps prevent vanishing gradients because the derivative of ReLU is 1 for all positive inputs, ensuring that gradients do not diminish as quickly.\n",
    "\n",
    "2- Use of Residual Networks: Architectures like ResNets introduce skip connections that allow gradients to flow through the network more directly, mitigating the vanishing problem by providing alternate pathways for gradient propagation.\n",
    "\n",
    "3- Batch Normalization: This technique normalizes the input layer by adjusting and scaling activations, which helps maintain a healthy gradient flow across deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: There is a neuron in the hidden layer that always results in an error.\n",
    "### Answer\n",
    " it suggests that there might be an issue with how the neuron is functioning or being integrated into the network. This can arise from various sources such as problems in the neuron's activation, issues with weight initialization, or errors in the data feeding into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: What do you understand by a computational graph?\n",
    "### Answer\n",
    "A computational graph is a directed graph where the nodes correspond to operations or variables. Variables can feed their value into operations, and operations can feed their output into other operations. This way, every node in the graph defines a function of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: What is Cross Entropy loss function\n",
    "### Answer\n",
    "Cross Entropy Loss function, also known as log loss, is a widely used performance metric for classification models, particularly in settings where the outputs are probabilities. It measures the performance of a classification model whose output is a probability value between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17: Why is Cross-entropy preferred as the cost function for multi-class classification problems?\n",
    "# Answer\n",
    "Because the decision boundary in a classification task is large (in comparison with regression).\n",
    "Cross-entropy is preferred as the cost function for multi-class classification problems due to its effectiveness in handling probabilities and its impact on the training process of a classifier. This preference stems from several key properties and advantages that make cross-entropy particularly suitable for tasks where predictions are inherently probabilistic and classes are mutually exclusive.\n",
    "\n",
    "1- Cross-entropy loss provides useful gradients during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qestion 20: What kind of loss function should we use for multi class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use multi-class ceoss entropy which is known SofMax function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example is in test_softmax.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
